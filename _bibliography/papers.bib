@inproceedings{cui_receive_2023,
      title={Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles}, 
      author={Can Cui and Yunsheng Ma and Xu Cao and Wenqian Ye and Ziran Wang},
      year = {2023},
      booktitle = {reviewing},
      arxiv={2310.08034},
      abbr = {under review},
      teaser = {cover_receive.png},
      selected = {true},
}


@inproceedings{cui_drive_2023,
      title={Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles}, 
      author={Can Cui and Yunsheng Ma and Xu Cao and Wenqian Ye and Ziran Wang},
      year = {2023},
      booktitle = {reviewing},
      arxiv={2309.10228},
      teaser = {cover_drive.png},
      abbr = {under review},
      selected = {true},
}

@inproceedings{cui_red_2023,
    author = {Cui, Can and Ma, Yunsheng and Lu, Juanwu and Wang, Ziran},
    title = {REDFormer: Radar Enlightens the Darkness of Camera Perception with Transformers},
    booktitle = {Proceedings of the IEEE Transactions on Intelligent Vehicles},
    year = {2023},
    abbr = {T-IV},
    selected = {true},
    teaser = {cover_red.png},
    abstract = {Enhancing the accuracy and reliability of perception systems in automated vehicles is critical, especially under varying driving conditions. Unfortunately, the challenges of adverse weather and low-visibility conditions can seriously degrade camera performance, introducing significant risks to vehicle safety. To address these concerns, in this study, we introduce a novel transformer-based 3D object detection model named `REDFormer'. By exploiting bird's-eye-view camera-radar fusion, the REDFormer offers a more practical and financially viable solution for tackling low-visibility conditions. We validate our model using the comprehensive nuScenes dataset, incorporating camera images, multi-radar point clouds, weather information, and time-of-day data. In comparative analyses, our model surpasses state-of-the-art benchmarks in both classification and detection accuracy. An in-depth ablation study further elucidates the individual contributions of each model component in overcoming the challenges posed by weather and lighting conditions. Experimental results specifically highlight the model's significant performance improvements, demonstrating a 31.31\% increase in accuracy under rainy conditions and a 46.99\% enhancement during nighttime scenarios, affirming REDFormer's potential as a robust and cost-effective solution for automated vehicles.},
}

@inproceedings{ma_macp_2024,
    author = {Yunsheng Ma and Juanwu Lu and Can Cui and Sicheng Zhao and Xu Cao and Wenqian Ye and Ziran Wang},
    title = {MACP: Efficient Model Adaptation for Cooperative Perception},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
    year = {2023},
    abbr = {WACV},
    selected = {true},
    teaser = {cover_macp.png},
    abstract = {Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to ``see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs.},
}



@inproceedings{cui_human_2023,
    author = {Can Cui and Yunsheng Ma and Xu Cao and Wenqian Ye and Ziran Wang},
    title = {Human-Autonomy Teaming on Autonomous Vehicles with Large Language Model-Enabled Human Digital Twins},
    booktitle = {Proceedings of the ACM/IEEE Symposium on Edge Computing},
    year = {2023},
    abbr = {SEC},
    selected = {true},
    teaser = {cover_human.png},
    abstract = {The development of autonomous vehicles is dramatically reshaping the transportation landscape, bringing new challenges and opportunities in human-machine interaction. As autonomous vehicles evolve, understanding and responding to human intent becomes significant, and therefore require new ways of human-autonomy teaming. A human digital twin (HDT) is a virtual representation of an individual driver, capturing their preferences, behaviors, and physiological states, enabling machines to better understand and predict human needs and responses. In this paper, we explore how large language models (LLMs), like GPT-4 and LLaMA, together with HDTs are changing the way humans team up with autonomous vehicles. These LLMs help make our conversations with vehicles more natural and intuitive. By pairing them in HDTs, we can get real-time feedback and smarter responses. This combination offers not just easier control but also safer driving experiences. We will break down how this works, why it matters, and what we might expect in the future.},
}


@article{cui_radar_2023,
    title = {Radar {Enlighten} the {Dark}: {Enhancing} {Low}-{Visibility} {Perception} for {Automated} {Vehicles} with {Camera}-{Radar} {Fusion}},
    author = {Cui, Can and Ma, Yunsheng and Lu, Juanwu and Wang, Ziran},
    journal = {IEEE International Conference on Intelligent Transportation Systems},
    arxiv = {2305.17318},
    year = {2023},
    selected = {true},
    abbr = {ITSC},
    teaser = {cover_redformer.png},
}